{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ABOUT THE ENVIRONMENT**\n",
    "\n",
    "**Software Requirements:** Numpy and OpenCV2\n",
    "\n",
    "Fundamentals of the environment:\n",
    "\n",
    "1. This environment is called Survival Gridworld\n",
    "2. Survival gridworld is a two-dimensional grid environment where obstacles and rewards are distributed between the start-point and the end-point.\n",
    "3. The task of the agent is to navigate a path from the start-point to end-point while keeping a positive Energy or Exploration Capacity.\n",
    "4. Starting Energy value should be defined.\n",
    "5. The agent is allowed to move in one of the four adjacent directions.\n",
    "6. Each movement in the gridworld costs a fixed Energy defined by the state trandition penalty δ (delta_s).\n",
    "7. Path Rewards: Positive values in the grid matrix increase the energy while negative values decreases it.\n",
    "\n",
    "The state is composed of four (4) channels:\n",
    "\n",
    "1. Current State - n x n (n should be odd) cut-out of the surroundings relative to the agent’s current position.\n",
    "2. Previous State - n x n (n should be odd) cut-out of the surroundings relative to the agent’s previous position.\n",
    "3. Location state - shows the position of the start-point and the goal with the agent’s current position as      well as the agent’s four previous positions.\n",
    "4. E State - shows the representation of the current Energy or the Exploration Capacity as a 2-D energy bar.\n",
    "\n",
    "Allowed actions:\n",
    "\n",
    "1. 0 - Up \n",
    "2. 1 - Down \n",
    "3. 2 - Left \n",
    "4. 3 - Right\n",
    "\n",
    "Reward function\n",
    "\n",
    "Let: ρ => Path rewards or Consumable rewards\n",
    "\n",
    "1. r = +1: If Success\n",
    "2. r = -1: If Failure\n",
    "3. r = -0.3: If agent bumps gridworld boundary\n",
    "4. r = ρ / (ρmax + σ); σ = 5: If ρ > 0\n",
    "5. r = ρ / (ρmin + σ); σ = 5: If ρ < 0\n",
    "6. (The constant σ is added to the denominator so that: +1 > r(ρmax), r(ρmin) > -1)\n",
    "\n",
    "Score function:\n",
    "\n",
    "1. If Success: score = max(1, (current_energy - initial_energy + 1))\n",
    "2. If Failure: score = 0\n",
    "\n",
    "Mechnics for Infinite Resource conversion:\n",
    "\n",
    "1. The Energy or Exploration Capacity only determines the score using the Score function but not the termination.\n",
    "2. If the Energy < 0 then Energy = 0.1.\n",
    "3. The agent does not terminate until Success condition or until it consumes any path_reward <= p_terminate (set by the user).\n",
    "4. To prevent being stuck indefinitely in a particular episode during training, the agent also terminates at max_steps set by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import environment as _env\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DEFAULT environment\n",
    "env = _env.Environment(is_default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RANDOM environment\n",
    "env = _env.Environment(is_default=False, grid_size=[14, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CUSTOM environment\n",
    "\n",
    "# Initialize default\n",
    "env = _env.Environment(is_default=True)\n",
    "\n",
    "# Sample custom gridworld matrix (12 x 12)\n",
    "gmatrix = [[   0,   5,   0,  -8, -10, -12,  0,   0,   0,  -12,   5,   0],\n",
    "           [   0,   0,   3, -13,   9,   0,   8,   0,  10,   0,   1,   1],\n",
    "           [   7,   0,   0,   0,   6,  -5,   2,   2,   4,   0,   3,   0],\n",
    "           [   0,   0,   0,   4, -17,   0, -17,  -7,   0,   0,   0,   0],\n",
    "           [ -10,   5, -18,   0, -12,   0,   0,  10, -20,   5,   0, -20],\n",
    "           [   0,  10,  -1,  -3,   0,   5,   0,   4,   0,   0,   8,   0],\n",
    "           [  -9, -20,  -8,   0,   5, -12,   3,   0,   0, -10, -20,  10],\n",
    "           [   0, -14,   9,   0,  -9, -20,   0,   6,   0, -20,   0,  -1],\n",
    "           [ -14,   0,  -4,   1,  -4,   2,   5,  -4,  10, -18, -20, -13],\n",
    "           [  -9,   0,   0,   2,   8, -12,   0, -14,   0, -20,   0,  -7],\n",
    "           [   0,  -7, -20,   0,   0,  -6,   0, -17,  -2,   0,  -8,   0],\n",
    "           [  -9,   0,  10,   0,   4,   0,   3,   0,   4,   0,   0,  10]]\n",
    "\n",
    "# Define custom environment\n",
    "env.custom_environment(gridmatrix=gmatrix, delta_s=1.4, start=[9, 11], end=[2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using INFINITE RESOURCE ENVIRONMENT for other experiment purposes\n",
    "\n",
    "# Initialize default\n",
    "env = _env.Environment(is_default=True)\n",
    "\n",
    "# Set as infinite resource gridworld and define termination parameter (terminate at <= p_terminate)\n",
    "# max_steps is for defining how many steps will the agent terminate if it cannot reach any terminal states\n",
    "# max_steps = None is setting to the default at 1.25 x grid_height x grid_width\n",
    "env.set_inf_resource(set_env=True, p_terminate=-10, max_steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the n x n cut-size for the Current State and the Previous State\n",
    "# Default nxn cut-out size = [5, 5]\n",
    "env.fstate_size = [7, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial energy or exploration capacity\n",
    "initial_e = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent interaction loop\n",
    "def run(initial_e, path_render=False):\n",
    "    \n",
    "    # Reset all environment parameters\n",
    "    state = env.reset(initial_e)\n",
    "    \n",
    "    step = 1\n",
    "    total_rewards = 0.0\n",
    "    \n",
    "    while(True):\n",
    "         \n",
    "        # Render the environment\n",
    "        if(True): env.render(delay=500)\n",
    "    \n",
    "        # Generate random action\n",
    "        action = random.randint(low=0, high=4)\n",
    "        \"\"\"IF A POLICY MODEL IS USED: action = policy_model(state)\"\"\"\n",
    "        \n",
    "        next_state, path_reward, reward, end_episode = env.step(action)\n",
    "        total_rewards += reward\n",
    "        # path reward is for adding or subtracting to the energy\n",
    "        # reward is for the reinforcement learning algorithm\n",
    "        \n",
    "        print(\"@step\", step, \n",
    "              \": path reward = \", path_reward, \n",
    "              \" reward = \", round(reward, 2))\n",
    "            \n",
    "        if(end_episode):\n",
    "            print(\"episode score = \", env.get_score(), \n",
    "                  \" total rewards = \", round(total_rewards, 2))\n",
    "            \n",
    "            # Render the environment at end episode\n",
    "            if(True): env.render(delay=500)\n",
    "                \n",
    "            # Render the entire path of the agent\n",
    "            if(path_render): env.path_render(delay=7000)\n",
    "            \n",
    "            env.close_render()\n",
    "            env.close_path_render()\n",
    "            break\n",
    "        else:\n",
    "            state = next_state # Update current state\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(initial_e, path_render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
